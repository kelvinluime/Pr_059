{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 Final Project - Waitz Library Traffic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background\n",
    "\n",
    "As students at UC San Diego we are presented with some obstacles in our weekly responsibility to study and concentrate towards our schoolwork. We have collectively run into anecdotal evidence that a major obstacle for the average UC San Diego student is finding an optimal place to study. Students often need a place to study or work and naturally head to Geisel Library only to find that space is not immediately available and that it may take a substantial chunk of time just to find a chair to sit on. This problem can directly impact a student’s time management, cut down study time and deteriorate their ability to thrive.\n",
    "    \n",
    "This obstacle is already being tackled by Geisel Library and companies like Waitz. Geisel Library has created a medium on their website that allows students to book group study rooms and to view which study rooms are currently occupied. This effort mitigates the obstacle of finding a study space for student groups if they are able to reserve a room well in advance. However, this solution does not meet the needs of the average student. It lacks the needed flexibility, on-the-run foresight, and only covers a small fraction of Geisel Library. \n",
    "\n",
    "In a further attempt to cover the needs of individual students on a daily basis, Waitz launched an initiative to provide students with an accurate, real time, readily accessible representation of Geisel Library’s “traffic”- or the density of population in the Library’s many spaces. Founded by UCSD alumni, the company’s current goal is to turn this tool into an app available to help students save time and effort in their search for proper study space. By extension, Waitz’s project could substantially promote student success. \n",
    "\n",
    "So far, Waitz has developed a website which informs users of the current population in each floor of Geisel Library. In order to meet their goal of limiting human congestion by helping students to navigate library traffic; Waitz must be able to cover more specific areas of Geisel Library and to have a more predictive model of traffic coming to and from its spaces. There are a few factors which may have a direct effect on Geisel Library’s use which may be successful predictors for a predictive model of library traffic. Our COGS108 team has joined Waitz’s efforts by tackling this problem through a short term internship. Our project involves identifying possible correlative factors, quantifying their relationship to library traffic and developing a predictive data-driven model. Waitz will support us by giving us access to their advanced data collection and by mentoring us in the process. A foundational portion of our data analysis includes a dataset with live feed of the library’s population provided by Waitz.\n",
    "\n",
    "The questions of interest are what trends can we see from the traffic of the library throughout time in a quarter and what external factors influence the traffic?\n",
    "\n",
    "We have hypotheses relating to external factors such as in the middle of the day and warmer temperature has a positive correlation to a higher population in Geisel. A higher number of people in Geisel when closer to midterms/final weeks (finals probably more). Observation of us UCSD students of seeing a lot of people in the library during this time. Less availability of study rooms and spaces in Geisel indicate that there will be more people in Geisel. There will be a dip in the number of students at Geisel prior to next section of the class.\n",
    "\n",
    "This question is important because it allows students to efficiently plan their study schedule. This will give them the opportunity to decide whether or not it is worth going to Geisel at a specific time. Since this saves students time from going to UCSD's Libraries, they will be able to focus on their other worries. \n",
    "\n",
    "For the Library traffic dataset, Waitz gathers our data through small hardware devices that pick up smartphone signals in the surrounding area. This gives a rough estimate on the number of students at a given location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question\n",
    "\n",
    "Can we predict the traffic in UCSD’s libraries using data encompassing weather, schedule, large-scale events, traffic, and shuttles in these areas? Can we then use a model of library traffic to predict when there is more available library space?\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "UCSD’s library traffic is affected by the weather, timing of exams, class schedules, large-scale events, and shuttle schedules. Therefore, by using data from weather forecasts, class schedules, event calendars, and live shuttle maps, we will predict when the traffic within the library. By extension, this will also allow us to predict when the library will have available study space for those looking to study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members:\n",
    "\n",
    "- A14709564 (Philip La)\n",
    "    - Contributed to Introduction and Background, Data Description, Data Cleaning/Pre-processing, Data Visualization, Data Analysis and Results, Privacy/Ethics Consideration, Conclusions and Discussion \n",
    "- A14782354\n",
    "- A92120441\n",
    "- A14697769\n",
    "- A12297371\n",
    "- A12753074"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dataset Name: Waitz**\n",
    "- Link to the dataset:\n",
    "https://drive.google.com/file/d/1uGPgA2lcjtHfPnOkWlTi4CUhu677tHz_/view?usp=sharing\n",
    "https://f3sszy41z4.execute-api.us-west-2.amazonaws.com/testing/get-hub-mapping\n",
    "- Number of observations: (All of last fall quarter + Half of spring quarter quarter) * ~1,000 observations per day\n",
    "\n",
    "This dataset is given by https://www.ucsdwaitz.com/ and allowed by Professor Voytek as we had to signed legal documents to get access to the full dataset which contains features of time and present signals of Geisel Library, Biomed Library, as well as other venues such as Price Center for 2017 Fall Quarter and 2018 Spring Quarter. The links to the dataset only give a sample of the data as well the full dataset is stored in a database (DynamoDB).\n",
    "\n",
    "- **Dataset Name: Weather by hour in San Diego Montgomery, CA**\n",
    "- Link to the dataset: https://drive.google.com/file/d/1AzCrS6SybPOwAXEuSdPDEoDVxWjaRhmb/view?usp=sharing\n",
    "- Number of observations: 11,000+\n",
    "\n",
    "This dataset shows the information about the temperature, dew point, and other weather conditions of San Diego Montgomery, CA. This data presents the temperature of the city per hour from the year 2017-current and we scrape all pages through last quarter to this quarter, day by day using this URL format: https://www.wunderground.com/history/airport/KMYF/2018/02/16/DailyHistory.html?req_city=San%20Diego%20Montgomery&req_state=CA&reqdb.zip=92123&reqdb.magic=4&reqdb.wmo=99999.\n",
    "\n",
    "- **Dataset Name: UCSD Shuttle**\n",
    "- Link to the dataset: https://drive.google.com/file/d/1JfoZWVrGFwW0ItEqW3eqvkuDr6AEr2qD/view?usp=sharing\n",
    "- Number of observations: 20,000+\n",
    "\n",
    "Shows the speed and capacity of each of UCSD's shuttles for the routes they take. The routes are Arriba Shuttle, Mesa Nueva Shuttle, North Campus Shuttle, SIO Shuttle, South Campus Shuttle, and West Campus Connector and we may also include arrival times for each stop as well for a given time.\n",
    "\n",
    "- **Dataset Name: Scheduling of Classes via time**\n",
    "- Link to the dataset: https://act.ucsd.edu/scheduleOfClasses/scheduleOfClassesStudent.htm\n",
    "- Number of observations: 16,000+\n",
    "\n",
    "For this dataset, we are counting the total number of UCSD students that are taking classes at a certain period of time. This dataset with measure per hour for Fall 2017, Spring 2017, and Winter 2018 and we got this data by scraping the UCSD's schedule of classes website.\n",
    "\n",
    "We plan to combine these datasets by seeing the correlation between and how these external factors of weather, class schedule and shuttle's data will influence the traffic to Geisel library and possibly other venues as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning/Pre-processing\n",
    "\n",
    "We first get data from Waitz by querying their database. Then we get weather, shuttle, and schedule of classes by scraping websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Waitz - Geisel Library Traffic for Fall Quarter 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database\n",
    "import requests\n",
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "\n",
    "# Processing\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "import json\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "# Time\n",
    "import datetime\n",
    "import time\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def file_exists(file_name):\n",
    "    my_file = Path(file_name)\n",
    "    return my_file.is_file()\n",
    "\n",
    "def ts_to_dt(ts):\n",
    "    date, time = ts.split()[0], ts.split()[1]\n",
    "    month, day, year = [int(x) for x in date.split(\"/\")]\n",
    "    hour, minute = [int(x) for x in time.split(\":\")[:2]]\n",
    "    return datetime.datetime(year, month, day, hour, minute)\n",
    "\n",
    "def dt_to_ts(dt):\n",
    "    return dt.strftime('%m/%d/%Y %H:%M')\n",
    "\n",
    "def start_and_end_dt(day):\n",
    "    start = dt_to_ts(day + datetime.timedelta(hours=7))\n",
    "    end = dt_to_ts(day + datetime.timedelta(hours=23, minutes=59))\n",
    "    return start, end\n",
    "\n",
    "def merge_dfs(df1, df2):\n",
    "    if len(df2) == 0:\n",
    "        return df1\n",
    "    if len(df1) == 0:\n",
    "        return df2\n",
    "    df3 = pd.merge(df1, df2, on='Time_Stamp')\n",
    "    df3['Present_Signals'] = df3['Present_Signals_x'] + df3['Present_Signals_y']\n",
    "    return df3[['Time_Stamp', 'Present_Signals']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'SectionDump' is for fall quarter 2017 and 'hubplacement-section-dump' is for winter quarter 2018\n",
    "# 'SectionDump' starts at 2017-9-28 and 'hubplacement-section-dump' starts at 2018-1-10\n",
    "\n",
    "# Load and clean Waitz's Geisel data\n",
    "def get_geisel_data_from_db(earliest, latest, table_selection):\n",
    "    # get basic info on hubs\n",
    "    hub_map_url = \"https://f3sszy41z4.execute-api.us-west-2.amazonaws.com/testing/get-hub-mapping\"\n",
    "    hubinfo = eval(requests.get(hub_map_url).text)\n",
    "\n",
    "    # connect to DB\n",
    "    hps_secdump = boto3.resource('dynamodb').Table(table_selection)\n",
    "\n",
    "    df = pd.DataFrame(hubinfo)\n",
    "    df = df.sort_values('Section')\n",
    "    df = df.drop('DropRSSI',axis=1)\n",
    "\n",
    "    # dataframe holds only hubs that are in geisel\n",
    "    df_geisel = df[df['Section'].str.contains(\"Floor\")]\n",
    "    sections = set(df_geisel['Section'])\n",
    "\n",
    "    AWS_READ_CAP_UNITS = 200\n",
    "\n",
    "    \"\"\"\n",
    "    Section Dump Export\n",
    "    \"\"\"\n",
    "    now_ts = lambda: datetime.datetime.now(tz=pytz.utc).astimezone(pytz.timezone(\"US/Pacific\"))\n",
    "    now_dt = lambda: datetime.datetime.now()\n",
    "    print (\"Started at: \", now_ts())\n",
    "    cell_start = now_dt()\n",
    "    print (\"=======================================\")\n",
    "\n",
    "    section_data = {}\n",
    "\n",
    "    day = earliest\n",
    "    unitsconsumed = 0 \n",
    "    top_counts = {}\n",
    "    while day <= latest:\n",
    "        ts = dt_to_ts(day)\n",
    "        start, end = start_and_end_dt(day)\n",
    "        dumpstart = now_dt()\n",
    "        section_data[day] = {}\n",
    "        print (\"\\nStarting Section dump for {} ({} days to go)\".format(ts, (latest - day).days))\n",
    "\n",
    "        for section in sections:\n",
    "            if unitsconsumed >= AWS_READ_CAP_UNITS - 10: # let the cap units rest with buffer space\n",
    "                time.sleep(1.5)\n",
    "                unitsconsumed = 0\n",
    "\n",
    "            resp = hps_secdump.query(\n",
    "                KeyConditionExpression=Key('Section_Name').eq(section) & Key('Time_Stamp').between(start, end),\n",
    "                ReturnConsumedCapacity=\"TOTAL\")\n",
    "\n",
    "            unitsconsumed += resp[\"ConsumedCapacity\"][\"CapacityUnits\"]\n",
    "\n",
    "            if resp[\"ResponseMetadata\"][\"HTTPStatusCode\"] != 200 or \"LastEvaluatedKey\" in resp:\n",
    "                print (\"{} hub on {} didn't return all items\".format(hubinfo[\"location\"], day))\n",
    "                print (\"Current unitsconsumed: {}\".format(unitsconsumed))\n",
    "                print (\"items returned: {}\".format(resp[\"Count\"]))\n",
    "                print (\"\\n\\nBefore Breaking: {} Total (successful) Queries\".format(queries))\n",
    "                break\n",
    "\n",
    "            result = resp[\"Items\"]\n",
    "            if len(result) > 0:\n",
    "                section_data[day][section] = pd.DataFrame(result)\n",
    "\n",
    "        print (\"{} dump took {}\".format(ts, now_dt() - dumpstart))\n",
    "        day += datetime.timedelta(days=1)\n",
    "\n",
    "        print (\"=======================================\")\n",
    "        print (\"Ended at: \", now_ts())\n",
    "        print (\"Duration: \", now_dt() - cell_start)\n",
    "    \n",
    "    return section_data\n",
    "    \n",
    "def combiner(earliest, latest, section_data):\n",
    "    \"\"\"\n",
    "    Combine all the sections\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Starting combiner\")\n",
    "\n",
    "    dfs = []\n",
    "    pre_dfs = []\n",
    "    \n",
    "    pool = ThreadPool(4) \n",
    "    \n",
    "    def reducer(section):\n",
    "        return reduce(merge_dfs, section, pd.DataFrame(columns=['Time_Stamp', 'Present_Signals']))\n",
    "    \n",
    "    day = earliest\n",
    "    while day <= latest:\n",
    "        section = section_data[day].values()\n",
    "        pre_dfs.append(section)\n",
    "        day += datetime.timedelta(days=1)\n",
    "    \n",
    "    dfs = pool.map(reducer, pre_dfs)\n",
    "    \n",
    "    geisel_df = pd.concat(dfs)\n",
    "    geisel_df.index = geisel_df['Time_Stamp'].apply(lambda ts: ts_to_dt(ts))\n",
    "    return geisel_df[['Present_Signals']]\n",
    "\n",
    "def get_geisel_df(earliest, latest, table_selection, quarter):\n",
    "    \"\"\"Caches geisel data into csv and only updates the cache when earliest or latest is change\"\"\"\n",
    "    geisel_df_file_name = \"geisel_\" + quarter + \".csv\"\n",
    "    geisel_time_file_name = \".geisel_\" + quarter + \"_time_last_update.json\"\n",
    "    \n",
    "    geisel_df = None\n",
    "    \n",
    "    if file_exists(geisel_time_file_name) == True:\n",
    "        d = json.load(open(geisel_time_file_name))\n",
    "        if earliest != ts_to_dt(d[\"earliest\"]) or latest != ts_to_dt(d[\"latest\"]):\n",
    "            section_data = get_geisel_data_from_db(earliest, latest, table_selection)\n",
    "            geisel_df = combiner(earliest, latest, section_data)\n",
    "            geisel_df.to_csv(geisel_df_file_name)\n",
    "        \n",
    "    d = {\"earliest\": dt_to_ts(earliest), \"latest\": dt_to_ts(latest)}\n",
    "    json.dump(d, open(geisel_time_file_name, \"w\"))\n",
    "    \n",
    "    df_geisel = pd.read_csv(geisel_df_file_name)\n",
    "    \n",
    "    df_geisel['Time_Stamp'] = pd.to_datetime(df_geisel['Time_Stamp'])\n",
    "    df_geisel.index = df_geisel['Time_Stamp']\n",
    "    \n",
    "    return df_geisel[['Present_Signals']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
